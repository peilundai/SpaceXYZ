{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from segmentation_models import PSPNet\n",
    "from segmentation_models import FPN\n",
    "from segmentation_models import Unet\n",
    "from segmentation_models.segmentation_models.backbones import get_preprocessing\n",
    "\n",
    "from keras import backend as K\n",
    "import keras\n",
    "\n",
    "import spacexyz\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# train_image_path = \"/scratch2/peilun/train_images/\"\n",
    "# train_label_path = \"/scratch2/peilun/train_labels/\"\n",
    "# val_image_path = \"/scratch2/peilun/val_images/\"\n",
    "# val_label_path = \"/scratch2/peilun/val_labels/\"\n",
    "\n",
    "train_image_path = \"/scratch2/peilun/originalImages_augmented\"\n",
    "train_label_path = \"/scratch2/peilun/GT_augmented\"\n",
    "val_image_path = \"/scratch2/peilun/val_images/\"\n",
    "val_label_path = \"/scratch2/peilun/val_labels/\"\n",
    "\n",
    "train_images = spacexyz.path2filelist(train_image_path)\n",
    "train_labels = spacexyz.path2filelist(train_label_path)\n",
    "val_images = spacexyz.path2filelist(val_image_path)\n",
    "val_labels = spacexyz.path2filelist(val_label_path)\n",
    "\n",
    "assert(len(train_images) == len(train_labels))\n",
    "assert(len(val_images) == len(val_labels))\n",
    "\n",
    "n_training = len(train_images)\n",
    "n_val = len(val_images)\n",
    "print(n_training)\n",
    "print(n_val)\n",
    "\n",
    "input_size = (1024, 1024, 3)\n",
    "output_size = (1024, 1024)\n",
    "\n",
    "n_classes = 1+7\n",
    "\n",
    "# initialize data\n",
    "X_train = np.zeros([n_training, *input_size]).astype(np.uint8)\n",
    "y_train = np.zeros([n_training, *output_size]).astype(np.uint8)\n",
    "\n",
    "X_val = np.zeros([n_val, *input_size]).astype(np.uint8)\n",
    "y_val = np.zeros([n_val, *output_size]).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in  680  training samples...\n",
      "0.1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19.20.21.22.23.24.25.26.27.28.29.30.31.32.33.34.35.36.37.38.39.40.41.42.43.44.45.46.47.48.49.50.51.52.53.54.55.56.57.58.59.60.61.62.63.64.65.66.67.68.69.70.71.72.73.74.75.76.77.78.79.80.81.82.83.84.85.86.87.88.89.90.91.92.93.94.95.96.97.98.99.100.101.102.103.104.105.106.107.108.109.110.111.112.113.114.115.116.117.118.119.120.121.122.123.124.125.126.127.128.129.130.131.132.133.134.135.136.137.138.139.140.141.142.143.144.145.146.147.148.149.150.151.152.153.154.155.156.157.158.159.160.161.162.163.164.165.166.167.168.169.170.171.172.173.174.175.176.177.178.179.180.181.182.183.184.185.186.187.188.189.190.191.192.193.194.195.196.197.198.199.200.201.202.203.204.205.206.207.208.209.210.211.212.213.214.215.216.217.218.219.220.221.222.223.224.225.226.227.228.229.230.231.232.233.234.235.236.237.238.239.240.241.242.243.244.245.246.247.248.249.250.251.252.253.254.255.256.257.258.259.260.261.262.263.264.265.266.267.268.269.270.271.272.273.274.275.276.277.278.279.280.281.282.283.284.285.286.287.288.289.290.291.292.293.294.295.296.297.298.299.300.301.302.303.304.305.306.307.308.309.310.311.312.313.314.315.316.317.318.319.320.321.322.323.324.325.326.327.328.329.330.331.332.333.334.335.336.337.338.339.340.341.342.343.344.345.346.347.348.349.350.351.352.353.354.355.356.357.358.359.360.361.362.363.364.365.366.367.368.369.370.371.372.373.374.375.376.377.378.379.380.381.382.383.384.385.386.387.388.389.390.391.392.393.394.395.396.397.398.399.400.401.402.403.404.405.406.407.408.409.410.411.412.413.414.415.416.417.418.419.420.421.422.423.424.425.426.427.428.429.430.431.432.433.434.435.436.437.438.439.440.441.442.443.444.445.446.447.448.449.450.451.452.453.454.455.456.457.458.459.460.461.462.463.464.465.466.467.468.469.470.471.472.473.474.475.476.477.478.479.480.481.482.483.484.485.486.487.488.489.490.491.492.493.494.495.496.497.498.499.500.501.502.503.504.505.506.507.508.509.510.511.512.513.514.515.516.517.518.519.520.521.522.523.524.525.526.527.528.529.530.531.532.533.534.535.536.537.538.539.540.541.542.543.544.545.546.547.548.549.550.551.552.553.554.555.556.557.558.559.560.561.562.563.564.565.566.567.568.569.570.571.572.573.574.575.576.577.578.579.580.581.582.583.584.585.586.587.588.589.590.591.592.593.594.595.596.597.598.599.600.601.602.603.604.605.606.607.608.609.610.611.612.613.614.615.616.617.618.619.620.621.622.623.624.625.626.627.628.629.630.631.632.633.634.635.636.637.638.639.640.641.642.643.644.645.646.647.648.649.650.651.652.653.654.655.656.657.658.659.660.661.662.663.664.665.666.667.668.669.670.671.672.673.674.675.676.677.678.679."
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############# Read in training dataset #############\n",
    "####################################################\n",
    "\n",
    "print(\"reading in \", n_training, \" training samples...\")\n",
    "for i in range(n_training):\n",
    "    print(i, end='.')\n",
    "    t_image = cv2.imread(join(train_image_path, train_images[i]))\n",
    "    t_label = cv2.imread(join(train_label_path, train_labels[i]))\n",
    "    X_train[i,:,:,:] = cv2.resize(t_image, input_size[:2])\n",
    "    y_train[i,:,:] = cv2.resize(t_label[:,:,0], output_size[:2], interpolation=cv2.INTER_NEAREST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes=n_classes, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in  20  eval samples...\n",
      "0.1.2.3.4.5.6.7.8.9.10.11.12.13.14.15.16.17.18.19."
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############# Read in validation dataset ###########\n",
    "####################################################\n",
    "\n",
    "print(\"reading in \", n_val, \" eval samples...\")\n",
    "for i in range(n_val):\n",
    "    print(i,end= '.')\n",
    "    v_image = cv2.imread(join(val_image_path, val_images[i]))\n",
    "    v_label = cv2.imread(join(val_label_path, val_labels[i]))\n",
    "    X_val[i,:,:,:] = cv2.resize(v_image, input_size[:2])\n",
    "    y_val[i,:,:] = cv2.resize(v_label[:,:,0], output_size[:2], interpolation=cv2.INTER_NEAREST)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes=n_classes, dtype='float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "############# Preprocess data ######################\n",
    "####################################################\n",
    "\n",
    "preprocessing_fn = get_preprocessing('resnet34')\n",
    "x = preprocessing_fn(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, test_data):\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x, y = self.test_data\n",
    "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
    "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "############# Set model parameters #################\n",
    "####################################################\n",
    "\n",
    "model = Unet(backbone_name='resnet34', classes=n_classes, activation='softmax')\n",
    "model.compile('Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 680 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      "100/680 [===>..........................] - ETA: 59s - loss: 0.0576 - categorical_accuracy: 0.9789"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-06fbfb022874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTestCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FPN_epoch10_roomtypes.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/peilun/envs/tensorflow_py36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/scratch2/peilun/envs/tensorflow_py36/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/peilun/envs/tensorflow_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/peilun/envs/tensorflow_py36/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch2/peilun/envs/tensorflow_py36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############# Training model #######################\n",
    "####################################################\n",
    "\n",
    "for i in range(10):\n",
    "    print('in iteration: ', i)\n",
    "    model.fit(x, y_train,  validation_data=(X_val, y_val), callbacks=[TestCallback((X_val, y_val))], batch_size=1, epochs=10, verbose=True)\n",
    "    model_name = 'Unet_epoch_'+str(10*(i+1))+'.h5'\n",
    "    model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "# pred = model.predict(X_val, batch_size=None, verbose=1, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbe87cf8978>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import imgaug as ia\n",
    "# from imgaug import augmenters as iaa\n",
    "\n",
    "# one_hot = spacexyz.onehot2ind(pred)\n",
    "\n",
    "# k=19\n",
    "# label = one_hot[k,:,:]\n",
    "# segmap = label.astype(np.int32)\n",
    "# segmap = ia.SegmentationMapOnImage(segmap, shape=(512, 512), nb_classes=1+6)\n",
    "# plt.imshow(segmap.draw_on_image(X_val[k,:,:,:]))\n",
    "# cv2.imwrite('messigray.png',segmap.draw_on_image(X_val[k,:,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_py36]",
   "language": "python",
   "name": "conda-env-tensorflow_py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
